{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama # newer version of this import from langchain_community.chat_models import ChatOllama idk if it changes anything though\n",
    "from langchain_core.prompts import ChatPromptTemplate # Prompt templates convert raw user input to better input to the LLM (guide reponse)\n",
    "from langchain_core.output_parsers import StrOutputParser # original model output is a message but this function parses it to a string (easier to work with)\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the database file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the JSON file\n",
    "with open(r'data/databaesDB.json', 'r') as f:\n",
    "    kdl_db = json.load(f)\n",
    "\n",
    "# Reading in the CSV file\n",
    "#df = pd.read_csv(r'data\\databasedb.csv', encoding=\"ANSI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For books that are in KDL's database (AKA have am index present in book_recs df) this LLM recommends them in relation to the original question! \n",
    "\n",
    "model = \"llama3.1\" # mistral, llama2, kdl_copilot_llama3, llama3, llama2:13b\n",
    "llm = Ollama(model=model, temperature=0)\n",
    "\n",
    "system = \"\"\"Your are a helpful librarian AI assistant. You Assist librarians with finding information about KDL databases relevant to a query . \\\n",
    "Given a json file of database names, their URLS, their associated descriptions, you will provide me the name of the database that you think is most reavlent to answer my query. \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system), \n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser # If our kdl db had a link to bibliocommons that would be sweet to add in here for easy staff access! \n",
    "\n",
    "u_input = \"I'm looking for information to help a student prepare for their driving test\"\n",
    "\n",
    "output = chain.invoke({\"input\": f\"Given the follow json file of KDL databases: {kdl_db}, recommend 3 databases based on this prompt: {u_input}.\"}) \n",
    "\n",
    "print(output) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
